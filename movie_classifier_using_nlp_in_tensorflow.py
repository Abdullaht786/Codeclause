# -*- coding: utf-8 -*-
"""Movie Classifier using NLP in tensorflow

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/movie-classifier-using-nlp-in-tensorflow-89e96705-7b04-4034-8529-8c371d453950.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240929/auto/storage/goog4_request%26X-Goog-Date%3D20240929T100449Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D9e73bc1775883127974c2c72c7d185b45bb2bce1d3557aa7eb6cbce79bdc02381f083d17e14e1ddb42e5a5fa59d5f6025581e376964a328e169eacec3e1cf0cf2e9de1366ef56ea27983d93f7a54244849c5eab44ebbd7148b51478f456d24ac1611d2c1d7b6f2786d2cb1f75e14dccddb3ac4f34a545cc2b5bc0829ecb13a705a6fa28fbf02ab2a4336c686522bba9cb3ada99f4ae61f0ed3452d2d17c57f3738b2846f982294ade1a95182471975b8a3c154fa82bdda102249bca74e62dbdcd22e6d45c1694d841faa2e7542f1462bf96abfa99047612e122f6ed45a237385203687fce7be7fc2851e74d15e42ea800167083086df997f8e39d160081628b8
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'movie-genre-prediction:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F3545508%2F6178340%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240929%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240929T100449Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dcd9dfdbfb4455e38806f9f11760a9f574986d0d225254f746f2f6076751dcf16de569490547aa84a2eea951d54325260ed39d156f637adac8e5ee6cc90485804210756f29f17592c43a77c2f9c5777bc4211e554835cff2ff1ad871ce0081b9b74bd09f7ed91ab8ee1896ca712136ccea102fa935128b12cfc3206486d12ecc76cd72ef00394b55d6620ed534c5ef277b338868f1a33a37845d0bd49d8c4b524588475e85c309f51050b9b5276b3206dd4591836dd633aede8dce7cb990abc5aa72f5c3d1ecf16554d76d4d936673d36d3354d14401a18fe395e8f064ded875f18964f9c611e0f1e10a77c89056439c6d09431a0b4f3b178defe33fb0a7ff4bd'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

"""# Data"""

# Load the data
import pandas as pd

movie_data = pd.read_csv("/kaggle/input/movie-genre-prediction/train.csv")

n_rows = movie_data.shape[0]
n_columns = movie_data.shape[1]

print("\nThere are {} rows in the dataset".format(n_rows))
print("\nThere are {} columns in the dataset".format(n_columns))

"""# Using a subset

I don't want to use all of this data because it would make training slow. I am going to use a subset of the data with 500 examples. To test my implementation.
"""

# Use just 500 training example

movie_subset = movie_data.iloc[:500,:]

n_rows = movie_subset.shape[0]
n_columns = movie_subset.shape[1]

print("\nThere are {} rows in the dataset".format(n_rows))
print("\nThere are {} columns in the dataset".format(n_columns))

movie_subset.head()

# The id and movie_name columns are unnecessary

movie_subset = movie_subset.drop(['id', 'movie_name'], axis=1)

movie_subset.head()

"""# Binary classification
Turn the problem into a binary classification problem.
"""

movie_subset.genre = (movie_subset.genre == 'fantasy')*1

movie_subset.head()

"""# Subsets"""

# Threshold for split
m = movie_subset.shape[0]
threshold = int(m*0.75)

X_train = list(movie_subset.synopsis[:threshold])
X_valid = list(movie_subset.synopsis[threshold:])

import numpy as np # The labels/target/classes are expected to be numpy arrays

y_train = np.array(movie_subset.genre[:threshold])
y_valid = np.array(movie_subset.genre[threshold:])

print('X training set has', len(X_train), "examples.")
print('y training set has shape', len(y_train), "examples.")
print('X validation set has', len(X_valid), "examples.")
print('y validation set has', len(y_valid), "examples.")

X_train[:2]

y_train[:2]

"""# Tokenization

Task 1: Tokenize `synopsis`

    Transform each word into a number.

Task 2: Encode the categorical variable `genre`

    Give a different number to each category.
    So label encoding would do the trick.

## Hyperparameters
"""

vocab_size = 5000
embedding_dim = 16
max_length = 120
trunc_type = 'post'
oov_tok = '<OOV>'

"""## `Tokenizer`"""

from tensorflow.keras.preprocessing.text import Tokenizer

# Create instance of the tokenizer
# I chose a low number of words to train fast and just test my implementation

tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok) # Adding a out of word token

# Encode the data

tokenizer.fit_on_texts(X_train)

# Dictionary {"word": encoding}

word_index = tokenizer.word_index

word_index_frame = pd.DataFrame(word_index.keys(), word_index.values(), columns=["words"])
word_index_frame.head()

"""## `text_to_sequence`

It creates a list of sequences. Where each sequence is encoded using the encoding stored in `word_index`.

### Traininig set
"""

# Turn a list of strings-sentences into
# a list containing a list-sentences-values
# where each sentence is split into words
# and each word is substituted by the index
# in word_index
# Replace every instance of a word in a sequence by it's encoding

training_sequences = tokenizer.texts_to_sequences(X_train)

print(training_sequences[0])

# Word and word-encoding side by side
words = movie_subset.synopsis[0].split()
word_encodings = training_sequences[0]

pd.DataFrame([words, word_encodings], index=['words', 'word-encodings']).iloc[:,:10]

"""### Validation set"""

validation_sequences = tokenizer.texts_to_sequences(X_valid)

"""## Padding
The classifier needs same length input sequences so I'm gonna add padding

### Training set
"""

# Imports function

from tensorflow.keras.preprocessing.sequence import pad_sequences


# Apply padding to output of text_to_sequences

padded = pad_sequences(training_sequences, maxlen=max_length, truncating=trunc_type)

# See result. It won't do much because there is only 1 sentence
padded

"""### Validation set"""

validation_padded = pad_sequences(validation_sequences, maxlen=max_length)

"""# Modeling"""

# Create model
import tensorflow as tf

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    tf.keras.layers.GlobalAveragePooling1D(), # Faster than Flatten
    tf.keras.layers.Dense(6, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.summary()

model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history = model.fit(padded,
                    y_train,
                    epochs=100,
                    validation_data=(validation_padded, y_valid))

model.evaluate(validation_padded, y_valid)

model.predict(validation_padded)[:6]

import matplotlib.pyplot as plt

# Plot utility
def plot_graphs(history, string):
    plt.plot(history.history[string])
    plt.plot(history.history['val_'+string])
    plt.xlabel("Epochs")
    plt.ylabel(string)
    plt.legend([string, 'val_'+string])
    plt.show()

# Plot the accuracy and loss
plot_graphs(history, "accuracy")
plot_graphs(history, "loss")

X_train_pred = model.predict(padded)

X_train_pred[:10]

y_real = y_train.copy()
y_pred = X_train_pred.copy()

y_true = pd.DataFrame(y_real.flatten(), columns=['y_true'])
y_pred = pd.DataFrame(y_pred.flatten(), columns=['y_pred'])

comparisons = pd.concat([y_pred, y_true], axis=1)

comparisons[comparisons.y_true==1]

X_valid_pred = model.predict(validation_padded)

y_real = y_valid.copy()
y_pred = X_train_pred.copy()

y_true = pd.DataFrame(y_real.flatten(), columns=['y_true'])
y_pred = pd.DataFrame(y_pred.flatten(), columns=['y_pred'])

comparisons = pd.concat([y_pred, y_true], axis=1)

comparisons[comparisons.y_true==1]

